{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Machine Learning Yearning: Learning curves Ensemble Vs Deepnet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import all librairies\n",
    "- import train 80.csv file and val/ create source and dataset\n",
    "- split every 10% (0.1 until 0.9)\n",
    "- train models: ensemble and deepnet\n",
    "- prediction on val 20%\n",
    "- AUC\n",
    "- add the deepnet curve\n",
    "- ensemble learning curves: training splitted set vs validation set/do the - same for the deepent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import bigml.api\n",
    "from bigml.api import BigML\n",
    "\n",
    "\n",
    "api= BigML (project='project/5d94a52e5a213962e2000331')\n",
    "\n",
    "# train_set = read_csv('train_dataset_80.csv')\n",
    "# validation_set = read_csv('val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the source and dataset for the train_dataset_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = api.create_source('train_dataset_80.csv')\n",
    "api.ok(train_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = api.create_dataset(train_source)\n",
    "api.ok(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the source and dataset for the val set (dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_source = api.create_source('val.csv')\n",
    "api.ok(val_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = api.create_dataset(val_source)\n",
    "api.ok(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Nb_data Vs AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function named sum_split_auc to compare the auc from different split (split_rate)\n",
    "def sum_split_auc(split_rate):\n",
    "    \n",
    "    # Split in function of the split_rate \n",
    "    train_set_splitted = api.create_dataset(train_dataset,\n",
    "        {\"name\": \"train_set_splitted\", \"sample_rate\": split_rate, \"seed\": \"my seed\"})\n",
    "    api.ok(train_set_splitted)\n",
    "    \n",
    "    #Train on the ensemble model\n",
    "    ensemble = api.create_ensemble(train_set_splitted, {\"objective_field\":\"SeriousDlqin2yrs\"})\n",
    "    api.ok(ensemble)\n",
    "    \n",
    "    #Prediction on the val_set\n",
    "    batch_prediction_val_set = api.create_batch_prediction(ensemble, val_dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_val_set)\n",
    "    \n",
    "    api.download_batch_prediction(batch_prediction_val_set,filename = 'Prediction_val_set_split.csv')\n",
    "    \n",
    "    #Define the dataframe with the csv\n",
    "    df = pd.read_csv('Prediction_val_set_split.csv', index_col = 0)\n",
    "    \n",
    "    #Calculate the AUC\n",
    "    pos = df.loc[df['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df.loc[df['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1  \n",
    "    return(x/y) #AUC formula\n",
    "    \n",
    "# Define the n = 1/10 (100/10, do ten time the split)/Create the table Perfomance: nb_data/AUC\n",
    "table = [sum_split_auc(n/10) for n in range(1,11)]\n",
    "plt.xlabel(\"nb_data\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Performance\")\n",
    "plt.plot(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC ensemble Vs AUC deepnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_split_auc(split_rate):\n",
    "    \n",
    "    # Split in function of the split_rate \n",
    "    train_set_splitted = api.create_dataset(train_dataset,\n",
    "        {\"name\": \"train_set_splitted\", \"sample_rate\": split_rate, \"seed\": \"my seed\"})\n",
    "    api.ok(train_set_splitted)\n",
    "    \n",
    "    #Train on the ensemble model\n",
    "    ensemble = api.create_ensemble(train_set_splitted, {\"objective_field\":\"SeriousDlqin2yrs\"})\n",
    "    api.ok(ensemble)\n",
    "    \n",
    "    #Prediction on the val_set_ensemble\n",
    "    batch_prediction_val_set_ensemble = api.create_batch_prediction(ensemble, val_dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_val_set_ensemble)\n",
    "    \n",
    "    api.download_batch_prediction(batch_prediction_val_set_ensemble,filename = 'Prediction_val_set_split_ensemble.csv')\n",
    "    \n",
    "    #Define the dataframe with the csv\n",
    "    df_ensemble = pd.read_csv('Prediction_val_set_split_ensemble.csv', index_col = 0)\n",
    "    \n",
    "    #Calculate the AUC_ensemble\n",
    "    pos = df_ensemble.loc[df_ensemble['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df_ensemble.loc[df_ensemble['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1 \n",
    "            auc_ensemble = x/y #AUC formula\n",
    "    print(auc_ensemble) \n",
    "\n",
    "    #Train on the deepnet model\n",
    "    deepnet = api.create_deepnet(train_set_splitted, {\"objective_field\":\"SeriousDlqin2yrs\"})\n",
    "    api.ok(deepnet)\n",
    "    \n",
    "    #Prediction on the val_set_deepnet\n",
    "    batch_prediction_val_set_deepnet = api.create_batch_prediction(deepnet, val_dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_val_set_deepnet)\n",
    "    \n",
    "    api.download_batch_prediction(batch_prediction_val_set_deepnet,filename = 'Prediction_val_set_split_deepnet.csv')\n",
    "    \n",
    "    #Define the dataframe with the deepnet_csv\n",
    "    df_deepnet = pd.read_csv('Prediction_val_set_split_deepnet.csv', index_col = 0)\n",
    "    \n",
    "    #Calculate the AUC_deepnet\n",
    "    pos = df_deepnet.loc[df_deepnet['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df_deepnet.loc[df_deepnet['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1 \n",
    "            auc_deepnet = x/y #AUC formula\n",
    "    print(auc_deepnet) \n",
    "    \n",
    "    return(auc_ensemble, auc_deepnet) #Return the two curves\n",
    "    \n",
    "    \n",
    "# Define the n = 1/10 (100/10, do ten time the split (from 1 to 11))/Create the table Perfomance: nb_data/AUC\n",
    "table = [sum_split_auc(n/10) for n in range(1,11)]\n",
    "plt.xlabel(\"nb_data\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Performance\")\n",
    "plt.plot(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC ensemble val_set vs training_split_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function named sum_split_auc to compare the auc from different split (split_rate)\n",
    "def sum_split_auc(split_rate):\n",
    "    \n",
    "    # Split in function of the split_rate \n",
    "    train_set_splitted = api.create_dataset(train_dataset,\n",
    "        {\"name\": \"train_set_splitted\", \"sample_rate\": split_rate, \"seed\": \"my seed\"})\n",
    "    api.ok(train_set_splitted)\n",
    "    \n",
    "    #Train on the ensemble model\n",
    "    ensemble = api.create_ensemble(train_set_splitted, {\"objective_field\":\"SeriousDlqin2yrs\"})\n",
    "    api.ok(ensemble)\n",
    "    \n",
    "    #Prediction on the training split set to have the training error curve\n",
    "    batch_prediction_training = api.create_batch_prediction(ensemble, train_set_splitted,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_training)\n",
    "    \n",
    "    api.download_batch_prediction(batch_prediction_training,filename = 'Prediction_training.csv')\n",
    "    \n",
    "    #Define the training_dataframe with the csv\n",
    "    df_training = pd.read_csv('Prediction_training.csv', index_col = 0)\n",
    "    \n",
    "    #Calculate the AUC df_training\n",
    "    pos = df_training.loc[df_training['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df_training.loc[df_training['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1  \n",
    "            auc_training= x/y #AUC formula\n",
    "    print(auc_training)\n",
    "    \n",
    "    #Prediction on the val_set\n",
    "    batch_prediction_val_set = api.create_batch_prediction(ensemble, val_dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_val_set)\n",
    "    \n",
    "    api.download_batch_prediction(batch_prediction_val_set,filename = 'Prediction_val_set_split.csv')\n",
    "    \n",
    "    #Define the val_dataframe with the csv\n",
    "    df_val = pd.read_csv('Prediction_val_set_split.csv', index_col = 0)\n",
    "    \n",
    "    #Calculate the AUC\n",
    "    pos = df_val.loc[df_val['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df_val.loc[df_val['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1 \n",
    "            auc_val = x/y #AUC formula\n",
    "    print(auc_val) \n",
    "    \n",
    "    return(auc_training, auc_val) #Return the two curves\n",
    "    \n",
    "# Define the n = 1/10 (100/10, do ten time the split)/Create the table Perfomance: nb_data/AUC\n",
    "table = [sum_split_auc(n/10) for n in range(1,11)]\n",
    "plt.xlabel(\"nb_data\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Ensemble training Vs val\")\n",
    "plt.plot(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## AUC deepnet val_set vs training_split_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create a function named sum_split_auc to compare the auc from different split (split_rate)\n",
    "def sum_split_auc(split_rate):\n",
    "    \n",
    "    # Split in function of the split_rate \n",
    "    train_set_splitted = api.create_dataset(train_dataset,\n",
    "        {\"name\": \"train_set_splitted\", \"sample_rate\": split_rate, \"seed\": \"my seed\"})\n",
    "    api.ok(train_set_splitted)\n",
    "    \n",
    "    #Train on the deepnet model\n",
    "    deepnet = api.create_deepnet(train_set_splitted, {\"objective_field\":\"SeriousDlqin2yrs\"})\n",
    "    api.ok(deepnet)\n",
    "    \n",
    "    #Prediction on the training split set to have the training error curve\n",
    "    batch_prediction_training = api.create_batch_prediction(deepnet, train_set_splitted,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_training)\n",
    "    \n",
    "    api.download_batch_prediction(batch_prediction_training,filename = 'Prediction_training_deepnet.csv')\n",
    "    \n",
    "    #Define the training_dataframe with the csv\n",
    "    df_training = pd.read_csv('Prediction_training_deepnet.csv', index_col = 0)\n",
    "    \n",
    "    #Calculate the AUC df_training\n",
    "    pos = df_training.loc[df_training['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df_training.loc[df_training['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1  \n",
    "            auc_training= x/y #AUC formula\n",
    "    print(auc_training)\n",
    "    \n",
    "    #Prediction on the val_set\n",
    "    batch_prediction_val_set = api.create_batch_prediction(deepnet, val_dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_val_set)\n",
    "    \n",
    "    api.download_batch_prediction(batch_prediction_val_set,filename = 'Prediction_val_deepnet.csv')\n",
    "    \n",
    "    #Define the val_dataframe with the csv\n",
    "    df_val = pd.read_csv('Prediction_val_deepnet.csv', index_col = 0)\n",
    "    \n",
    "    #Calculate the AUC\n",
    "    pos = df_val.loc[df_val['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df_val.loc[df_val['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1 \n",
    "            auc_val = x/y #AUC formula\n",
    "    print(auc_val) \n",
    "    \n",
    "    return(auc_training, auc_val) #Return the two curves\n",
    "    \n",
    "# Define the n = 1/10 (100/10, do ten time the split)/Create the table Perfomance: nb_data/AUC\n",
    "table = [sum_split_auc(n/10) for n in range(1,11)]\n",
    "plt.xlabel(\"nb_data\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Deepenet training Vs val\")\n",
    "plt.plot(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing code for learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### Split the train_set in function of the split_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "#Crate function for splitting every 10% \n",
    "def split_train(split_rate):\n",
    "\n",
    "    train_set_splitted = api.create_dataset(train_dataset,{\"name\": \"train_set_splitted\", \"sample_rate\": split_rate, \"seed\": \"my seed\"})\n",
    "    api.ok(train_set_splitted)\n",
    "    \n",
    "    return train_set_splitted\n",
    "\n",
    "# Split (n/100)\n",
    "train_10 = split_train(10/100)\n",
    "train_20 = split_train(20/100)\n",
    "train_30 = split_train(30/100)\n",
    "train_40 = split_train(40/100)\n",
    "train_50 = split_train(50/100)\n",
    "train_60 = split_train(60/100)\n",
    "train_70 = split_train(70/100)\n",
    "train_80 = split_train(80/100)\n",
    "train_90 = split_train(90/100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the ensemble_model on a dataset\n",
    "def ensemble_model(dataset):\n",
    "    \n",
    "    ensemble = api.create_ensemble(dataset, {\"objective_field\":\"SeriousDlqin2yrs\"})\n",
    "    api.ok(ensemble)\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "# Definition of datasets used to train the ensemble\n",
    "ensemble_10 = ensemble_model(train_10)\n",
    "ensemble_20 = ensemble_model(train_20)\n",
    "ensemble_30 = ensemble_model(train_30)\n",
    "ensemble_40 = ensemble_model(train_40)\n",
    "ensemble_50 = ensemble_model(train_50)\n",
    "ensemble_60 = ensemble_model(train_60)\n",
    "ensemble_70 = ensemble_model(train_70)\n",
    "ensemble_80 = ensemble_model(train_80)\n",
    "ensemble_90 = ensemble_model(train_90)\n",
    "ensemble_100 = ensemble_model(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the deepnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the deepnet_model on a dataset\n",
    "def deepnet_model(dataset):\n",
    "    \n",
    "    deepnet = api.create_deepnet(dataset, {\"objective_field\":\"SeriousDlqin2yrs\"})\n",
    "    api.ok(deepnet)\n",
    "    \n",
    "    return deepnet\n",
    "\n",
    "# Definition of datasets used to train the deepnet\n",
    "deepnet_10 = deepnet_model(train_10)\n",
    "deepnet_20 = deepnet_model(train_20)\n",
    "deepnet_30 = deepnet_model(train_30)\n",
    "deepnet_40 = deepnet_model(train_40)\n",
    "deepnet_50 = deepnet_model(train_50)\n",
    "deepnet_60 = deepnet_model(train_60)\n",
    "deepnet_70 = deepnet_model(train_70)\n",
    "deepnet_80 = deepnet_model(train_80)\n",
    "deepnet_90 = deepnet_model(train_90)\n",
    "deepnet_100 = deepnet_model(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the val_set after deepnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on the val set using all the splitted train trained with deepnet\n",
    "def prediction_val_deepnet(model, dataset):\n",
    "    \n",
    "    batch_prediction_val_set_deepnet = api.create_batch_prediction(model, dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_val_set_deepnet)\n",
    "    \n",
    "    return batch_prediction_val_set_deepnet\n",
    "\n",
    "# Definition of the 'model' datasets\n",
    "batch_prediction_val_set_deepnet_10 = prediction_val_deepnet(deepnet_10, val_dataset)\n",
    "batch_prediction_val_set_deepnet_20 = prediction_val_deepnet(deepnet_20, val_dataset)\n",
    "batch_prediction_val_set_deepnet_30 = prediction_val_deepnet(deepnet_30, val_dataset)\n",
    "batch_prediction_val_set_deepnet_40 = prediction_val_deepnet(deepnet_40, val_dataset)\n",
    "batch_prediction_val_set_deepnet_50 = prediction_val_deepnet(deepnet_50, val_dataset)\n",
    "batch_prediction_val_set_deepnet_60 = prediction_val_deepnet(deepnet_60, val_dataset)\n",
    "batch_prediction_val_set_deepnet_70 = prediction_val_deepnet(deepnet_70, val_dataset)\n",
    "batch_prediction_val_set_deepnet_80 = prediction_val_deepnet(deepnet_80, val_dataset)\n",
    "batch_prediction_val_set_deepnet_90 = prediction_val_deepnet(deepnet_90, val_dataset)\n",
    "batch_prediction_val_set_deepnet_100 = prediction_val_deepnet(deepnet_100, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the prediction on val set using deepnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_10,filename = 'Prediction_val_deepnet_10.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_20,filename = 'Prediction_val_deepnet_20.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_30,filename = 'Prediction_val_deepnet_30.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_40,filename = 'Prediction_val_deepnet_40.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_50,filename = 'Prediction_val_deepnet_50.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_60,filename = 'Prediction_val_deepnet_60.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_70,filename = 'Prediction_val_deepnet_70.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_80,filename = 'Prediction_val_deepnet_80.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_90,filename = 'Prediction_val_deepnet_90.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_deepnet_100,filename = 'Prediction_val_deepnet_100.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the val_set after ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on the val set using all the splitted train trained with ensemble\n",
    "def prediction_val_ensemble(model, dataset):\n",
    "    \n",
    "    batch_prediction_val_set_ensemble = api.create_batch_prediction(model, dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_val_set_ensemble)\n",
    "    \n",
    "    return batch_prediction_val_set_ensemble\n",
    "\n",
    "# Definition of the 'model' datasets\n",
    "batch_prediction_val_set_ensemble_10 = prediction_val_ensemble(ensemble_10, val_dataset)\n",
    "batch_prediction_val_set_ensemble_20 = prediction_val_ensemble(ensemble_20, val_dataset)\n",
    "batch_prediction_val_set_ensemble_30 = prediction_val_ensemble(ensemble_30, val_dataset)\n",
    "batch_prediction_val_set_ensemble_40 = prediction_val_ensemble(ensemble_40, val_dataset)\n",
    "batch_prediction_val_set_ensemble_50 = prediction_val_ensemble(ensemble_50, val_dataset)\n",
    "batch_prediction_val_set_ensemble_60 = prediction_val_ensemble(ensemble_60, val_dataset)\n",
    "batch_prediction_val_set_ensemble_70 = prediction_val_ensemble(ensemble_70, val_dataset)\n",
    "batch_prediction_val_set_ensemble_80 = prediction_val_ensemble(ensemble_80, val_dataset)\n",
    "batch_prediction_val_set_ensemble_90 = prediction_val_ensemble(ensemble_90, val_dataset)\n",
    "batch_prediction_val_set_ensemble_100 = prediction_val_ensemble(ensemble_100, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the prediction on val set using ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_10,filename = 'Prediction_val_ensemble_10.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_20,filename = 'Prediction_val_ensemble_20.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_30,filename = 'Prediction_val_ensemble_30.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_40,filename = 'Prediction_val_ensemble_40.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_50,filename = 'Prediction_val_ensemble_50.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_60,filename = 'Prediction_val_ensemble_60.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_70,filename = 'Prediction_val_ensemble_70.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_80,filename = 'Prediction_val_ensemble_80.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_90,filename = 'Prediction_val_ensemble_90.csv')\n",
    "api.download_batch_prediction(batch_prediction_val_set_ensemble_100,filename = 'Prediction_val_ensemble_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the train splitted set after ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on the splitted train set using their splitted train trained with ensemble\n",
    "def prediction_train_ensemble(model, dataset):\n",
    "    \n",
    "    batch_prediction_training_ensemble = api.create_batch_prediction(model, dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_training_ensemble)\n",
    "    \n",
    "    return batch_prediction_training_ensemble\n",
    "\n",
    "# Definition of the datasets\n",
    "batch_prediction_training_ensemble_10 = prediction_train_ensemble(ensemble_10, train_10)\n",
    "batch_prediction_training_ensemble_20 = prediction_train_ensemble(ensemble_20, train_20)\n",
    "batch_prediction_training_ensemble_30 = prediction_train_ensemble(ensemble_30, train_30)\n",
    "batch_prediction_training_ensemble_40 = prediction_train_ensemble(ensemble_40, train_40)\n",
    "batch_prediction_training_ensemble_50 = prediction_train_ensemble(ensemble_50, train_50)\n",
    "batch_prediction_training_ensemble_60 = prediction_train_ensemble(ensemble_60, train_60)\n",
    "batch_prediction_training_ensemble_70 = prediction_train_ensemble(ensemble_70, train_70)\n",
    "batch_prediction_training_ensemble_80 = prediction_train_ensemble(ensemble_80, train_80)\n",
    "batch_prediction_training_ensemble_90 = prediction_train_ensemble(ensemble_90, train_90)\n",
    "batch_prediction_training_ensemble_100 = prediction_train_ensemble(ensemble_100, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download prediction train splitted trained with ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.download_batch_prediction(batch_prediction_training_ensemble_10,filename = 'Prediction_train_ensemble_10.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_20,filename = 'Prediction_train_ensemble_20.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_30,filename = 'Prediction_train_ensemble_30.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_40,filename = 'Prediction_train_ensemble_40.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_50,filename = 'Prediction_train_ensemble_50.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_60,filename = 'Prediction_train_ensemble_60.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_70,filename = 'Prediction_train_ensemble_70.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_80,filename = 'Prediction_train_ensemble_80.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_90,filename = 'Prediction_train_ensemble_90.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_ensemble_100,filename = 'Prediction_train_ensemble_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on train splitted set trained with deepnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_train_deepnet(model, dataset):\n",
    "    \n",
    "    batch_prediction_training_deepnet = api.create_batch_prediction(model, dataset,\n",
    "        {\"header\": True, \"all_fields\": True,\"prediction_name\": \"my_prediction\", \"probabilities\": True})\n",
    "    api.ok(batch_prediction_training_deepnet)\n",
    "    \n",
    "    return batch_prediction_training_deepnet\n",
    "\n",
    "# Definition of the datasets\n",
    "batch_prediction_training_deepnet_10 = prediction_train_deepnet(deepnet_10, train_10)\n",
    "batch_prediction_training_deepnet_20 = prediction_train_deepnet(deepnet_20, train_20)\n",
    "batch_prediction_training_deepnet_30 = prediction_train_deepnet(deepnet_30, train_30)\n",
    "batch_prediction_training_deepnet_40 = prediction_train_deepnet(deepnet_40, train_40)\n",
    "batch_prediction_training_deepnet_50 = prediction_train_deepnet(deepnet_50, train_50)\n",
    "batch_prediction_training_deepnet_60 = prediction_train_deepnet(deepnet_60, train_60)\n",
    "batch_prediction_training_deepnet_70 = prediction_train_deepnet(deepnet_70, train_70)\n",
    "batch_prediction_training_deepnet_80 = prediction_train_deepnet(deepnet_80, train_80)\n",
    "batch_prediction_training_deepnet_90 = prediction_train_deepnet(deepnet_90, train_90)\n",
    "batch_prediction_training_deepnet_100 = prediction_train_deepnet(deepnet_100, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download prediction on train split set trained with deepnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.download_batch_prediction(batch_prediction_training_deepnet_10,filename = 'Prediction_train_deepnet_10.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_20,filename = 'Prediction_train_deepnet_20.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_30,filename = 'Prediction_train_deepnet_30.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_40,filename = 'Prediction_train_deepnet_40.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_50,filename = 'Prediction_train_deepnet_50.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_60,filename = 'Prediction_train_deepnet_60.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_70,filename = 'Prediction_train_deepnet_70.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_80,filename = 'Prediction_train_deepnet_80.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_90,filename = 'Prediction_train_deepnet_90.csv')\n",
    "api.download_batch_prediction(batch_prediction_training_deepnet_100,filename = 'Prediction_train_deepnet_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC sum and Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for AUC sum\n",
    "def auc_sum(df):\n",
    "    pos = df.loc[df['SeriousDlqin2yrs'] == 1]\n",
    "    neg = df.loc[df['SeriousDlqin2yrs'] == 0] \n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for threshold_pos in pos['1 probability']: \n",
    "        for threshold_neg in neg['1 probability']:\n",
    "            if threshold_pos > threshold_neg:\n",
    "                x += 1 \n",
    "            y += 1  \n",
    "            auc = x/y \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC Ensemble on validation set\n",
    "- definition of dataframes and show AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_e_10 = pd.read_csv('Prediction_val_ensemble_10.csv', index_col = 0)\n",
    "df_val_e_20 = pd.read_csv('Prediction_val_ensemble_20.csv', index_col = 0)\n",
    "df_val_e_30 = pd.read_csv('Prediction_val_ensemble_30.csv', index_col = 0)\n",
    "df_val_e_40 = pd.read_csv('Prediction_val_ensemble_40.csv', index_col = 0)\n",
    "df_val_e_50 = pd.read_csv('Prediction_val_ensemble_50.csv', index_col = 0)\n",
    "df_val_e_60 = pd.read_csv('Prediction_val_ensemble_60.csv', index_col = 0)\n",
    "df_val_e_70 = pd.read_csv('Prediction_val_ensemble_70.csv', index_col = 0)\n",
    "df_val_e_80 = pd.read_csv('Prediction_val_ensemble_80.csv', index_col = 0)\n",
    "df_val_e_90 = pd.read_csv('Prediction_val_ensemble_90.csv', index_col = 0)\n",
    "df_val_e_100 = pd.read_csv('Prediction_val_ensemble_100.csv', index_col = 0)\n",
    "\n",
    "auc_val_e_set = [auc_sum(df_val_e_10),\n",
    "                 auc_sum(df_val_e_20),\n",
    "                 auc_sum(df_val_e_30),\n",
    "                 auc_sum(df_val_e_40),\n",
    "                 auc_sum(df_val_e_50),\n",
    "                 auc_sum(df_val_e_60),\n",
    "                 auc_sum(df_val_e_70),\n",
    "                 auc_sum(df_val_e_80),\n",
    "                 auc_sum(df_val_e_90),\n",
    "                 auc_sum(df_val_e_100)\n",
    "                ]\n",
    "print(auc_val_e_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC Deepnet on validation set\n",
    "- definition of dataframes and show AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_d_10 = pd.read_csv('Prediction_val_deepnet_10.csv', index_col = 0)\n",
    "df_val_d_20 = pd.read_csv('Prediction_val_deepnet_20.csv', index_col = 0)\n",
    "df_val_d_30 = pd.read_csv('Prediction_val_deepnet_30.csv', index_col = 0)\n",
    "df_val_d_40 = pd.read_csv('Prediction_val_deepnet_40.csv', index_col = 0)\n",
    "df_val_d_50 = pd.read_csv('Prediction_val_deepnet_50.csv', index_col = 0)\n",
    "df_val_d_60 = pd.read_csv('Prediction_val_deepnet_60.csv', index_col = 0)\n",
    "df_val_d_70 = pd.read_csv('Prediction_val_deepnet_70.csv', index_col = 0)\n",
    "df_val_d_80 = pd.read_csv('Prediction_val_deepnet_80.csv', index_col = 0)\n",
    "df_val_d_90 = pd.read_csv('Prediction_val_deepnet_90.csv', index_col = 0)\n",
    "df_val_d_100 = pd.read_csv('Prediction_val_deepnet_100.csv', index_col = 0)\n",
    "\n",
    "auc_val_d_set = [auc_sum(df_val_d_10),\n",
    "                 auc_sum(df_val_d_20),\n",
    "                 auc_sum(df_val_d_30),\n",
    "                 auc_sum(df_val_d_40),\n",
    "                 auc_sum(df_val_d_50),\n",
    "                 auc_sum(df_val_d_60),\n",
    "                 auc_sum(df_val_d_70),\n",
    "                 auc_sum(df_val_d_80),\n",
    "                 auc_sum(df_val_d_90),\n",
    "                 auc_sum(df_val_d_100)\n",
    "                ]\n",
    "print(auc_val_d_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC Deepnet on training set\n",
    "- definition of dataframes and show AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_d_10 = pd.read_csv('Prediction_train_deepnet_10.csv', index_col = 0)\n",
    "df_train_d_20 = pd.read_csv('Prediction_train_deepnet_20.csv', index_col = 0)\n",
    "df_train_d_30 = pd.read_csv('Prediction_train_deepnet_30.csv', index_col = 0)\n",
    "df_train_d_40 = pd.read_csv('Prediction_train_deepnet_40.csv', index_col = 0)\n",
    "df_train_d_50 = pd.read_csv('Prediction_train_deepnet_50.csv', index_col = 0)\n",
    "df_train_d_60 = pd.read_csv('Prediction_train_deepnet_60.csv', index_col = 0)\n",
    "df_train_d_70 = pd.read_csv('Prediction_train_deepnet_70.csv', index_col = 0)\n",
    "df_train_d_80 = pd.read_csv('Prediction_train_deepnet_80.csv', index_col = 0)\n",
    "df_train_d_90 = pd.read_csv('Prediction_train_deepnet_90.csv', index_col = 0)\n",
    "df_train_d_100 = pd.read_csv('Prediction_train_deepnet_100.csv', index_col = 0)\n",
    "\n",
    "auc_train_d_set = [auc_sum(df_train_d_10),\n",
    "                 auc_sum(df_train_d_20),\n",
    "                 auc_sum(df_train_d_30),\n",
    "                 auc_sum(df_train_d_40),\n",
    "                 auc_sum(df_train_d_50),\n",
    "                 auc_sum(df_train_d_60),\n",
    "                 auc_sum(df_train_d_70),\n",
    "                 auc_sum(df_train_d_80),\n",
    "                 auc_sum(df_train_d_90),\n",
    "                 auc_sum(df_train_d_100)\n",
    "                ]\n",
    "print(auc_train_d_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC Ensemble on training set\n",
    "- definition of dataframes and show AUC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_e_10 = pd.read_csv('Prediction_train_ensemble_10.csv', index_col = 0)\n",
    "df_train_e_20 = pd.read_csv('Prediction_train_ensemble_20.csv', index_col = 0)\n",
    "df_train_e_30 = pd.read_csv('Prediction_train_ensemble_30.csv', index_col = 0)\n",
    "df_train_e_40 = pd.read_csv('Prediction_train_ensemble_40.csv', index_col = 0)\n",
    "df_train_e_50 = pd.read_csv('Prediction_train_ensemble_50.csv', index_col = 0)\n",
    "df_train_e_60 = pd.read_csv('Prediction_train_ensemble_60.csv', index_col = 0)\n",
    "df_train_e_70 = pd.read_csv('Prediction_train_ensemble_70.csv', index_col = 0)\n",
    "df_train_e_80 = pd.read_csv('Prediction_train_ensemble_80.csv', index_col = 0)\n",
    "df_train_e_90 = pd.read_csv('Prediction_train_ensemble_90.csv', index_col = 0)\n",
    "df_train_e_100 = pd.read_csv('Prediction_train_ensemble_100.csv', index_col = 0)\n",
    "\n",
    "auc_train_e_set = [auc_sum(df_train_e_10),\n",
    "                 auc_sum(df_train_e_20),\n",
    "                 auc_sum(df_train_e_30),\n",
    "                 auc_sum(df_train_e_40),\n",
    "                 auc_sum(df_train_e_50),\n",
    "                 auc_sum(df_train_e_60),\n",
    "                 auc_sum(df_train_e_70),\n",
    "                 auc_sum(df_train_e_80),\n",
    "                 auc_sum(df_train_e_90),\n",
    "                 auc_sum(df_train_e_100)\n",
    "                ]\n",
    "print(auc_train_e_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph perfomance of Ensemble model on validation set depending on the train set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Train_set size\")\n",
    "plt.ylabel(\"Performance : AUC\")\n",
    "plt.title(\"Learning_Curves - Ensemble\")\n",
    "plt.grid()\n",
    "plt.axhline(y=0.87, color = 'green', label=\"Reference Performance\")\n",
    "plt.ylim(0.75,0.9)\n",
    "plt.plot(auc_val_e_set, label= \"Ensemble\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.rcParams['figure.figsize'] = [15 , 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph perfomances of Ensemble vs Deepnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Train_set size\")\n",
    "plt.ylabel(\"Performance : AUC\")\n",
    "plt.title(\"Learning_Curves - Ensemble vs deepnet\")\n",
    "plt.grid()\n",
    "plt.axhline(y=0.87, color = 'green', label=\"Reference Performance\")\n",
    "plt.ylim(0.75,1)\n",
    "plt.plot(auc_val_e_set, label= \"Ensemble\")\n",
    "plt.plot(auc_val_d_set, label= \"Deepnet\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams['figure.figsize'] = [15 , 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph performances of Ensemble training vs validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Train_set size\")\n",
    "plt.ylabel(\"Performance : AUC\")\n",
    "plt.title(\"Learning_Curves - Ensemble training vs validation\")\n",
    "plt.grid()\n",
    "plt.axhline(y=0.87, color = 'green', label=\"Reference Performance\")\n",
    "plt.ylim(0.75,1)\n",
    "plt.plot(auc_val_e_set, label= \"Ensemble val set\")\n",
    "plt.plot(auc_train_e_set, label= \"Ensemble train set\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams['figure.figsize'] = [15 , 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph performances of Deepnet training vs validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Train_set size\")\n",
    "plt.ylabel(\"Performance : AUC\")\n",
    "plt.title(\"Learning_Curves - Deepnet training vs validation\")\n",
    "plt.grid()\n",
    "plt.axhline(y=0.87, color = 'green', label=\"Reference Performance\")\n",
    "plt.ylim(0.75,0.9)\n",
    "plt.plot(auc_val_d_set, label= \"Deepnet val set\")\n",
    "plt.plot(auc_train_d_set, label= \"Deepnet train set\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.rcParams['figure.figsize'] = [15 , 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
